import cv2
import numpy as np
import glob
import pickle
import pandas as pd
import os
from constants import IMAGE_SIZE, TRAIN_DATA_PATH, TRAIN_DATA_FILE
from common import summary_labels, GPSz


# For accepted [1,0]
# For rejected [0,1]

def create_data(job_dir, job, datafile):

    #build the job and selection file name
    model_filename = job_dir+'/'+job+'-model.csv'
    selection_filename= job_dir+'/'+job+'-selection.csv'

    #cache the data from those files
    metadata = pd.read_csv(model_filename, sep=',')
    metadata = metadata.drop(metadata.columns[0], 1)
    metadata = metadata.as_matrix()
    selection = pd.read_csv(selection_filename, sep=',')
    selection = selection.drop(selection.columns[0], 1)
    selection = selection.as_matrix()

    image_size = IMAGE_SIZE
    length = image_size * image_size

    pics = glob.glob(job_dir+'/'+job+'*.png')
    total_images = len(pics)

    array_size = length +2 + metadata.shape[1]
    main_data = np.zeros([total_images, array_size], dtype='f')

    #read images and fill the databuffer
    i = 0
    for pic in pics:

        img = cv2.imread(pic, 0)
        orig_img_len = np.size(img, 0)  #rows and colums should be the same
        img = cv2.resize(img, (image_size, image_size), interpolation=cv2.INTER_AREA)
        resized_img_len =  np.size(img, 0)
        #print (orig_img_len, resized_img_len)
        main_data[i, 0:length] = np.reshape(img, (1, length))

        # find the class
        start = pic.find('-classes-') + len('-classes-')
        if pic.find('_') == -1:
            #not a file generated by augmentation
            end = pic.find('.')
        else:
            #file generted by keras augmentation. introduces _x_ into the file name
            end = pic.find('_')

        class_num = int(pic[start:end:])

        if selection[class_num-1] == 1:
            #accepted
            main_data[i, length] = 1
        else:
            if selection[class_num-1] == 0:
                #rejected
                main_data[i, length + 1] = 1
            else:
                print ('Houston, we have a problem!!!')

        #stuff meta data
        # adjust pixel size
        if resized_img_len != orig_img_len:
            metadata[class_num - 1][summary_labels.index(GPSz)] = \
                metadata [class_num-1][summary_labels.index(GPSz)]*orig_img_len/resized_img_len
        else:
            pass

        main_data[i, length+2:] = metadata [class_num-1]
        i = i + 1

    np.random.shuffle(main_data)
    pickle.dump (main_data, datafile)
    return

def setup_data():
    #zap existing files
    file = open(TRAIN_DATA_FILE, 'wb')
    file.close()

    #Traverse the directory for csv files: model and selection
    rootpath = TRAIN_DATA_PATH
    rootDir = os.getcwd() + '/' + rootpath
    for dirName, subdirList, fileList in os.walk(rootDir):
        for subdir in subdirList:
            file = open(TRAIN_DATA_FILE, 'ab')
            create_data(dirName+'/'+subdir, subdir, file)
            file.close()


if __name__ == '__main__':
	setup_data()
